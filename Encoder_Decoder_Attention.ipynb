{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Encoder - Decoder - Attention.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOf5vyN+1XANn9W7ysbjTG1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-l6sdOBWFz8"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import unicodedata\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz9wAZI3WUk2",
        "outputId": "51944ed3-8e93-45ec-e84c-cc2f6a2789f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n",
            "2654208/2638744 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
      ],
      "metadata": {
        "id": "cNnwE6UDXJj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "Knju6GJaXsC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "metadata": {
        "id": "wKNT1VE5Xj2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "  return zip(*word_pairs)\n",
        "\n",
        "en, sp = create_dataset(path_to_file, None)\n",
        "\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer\n",
        "\n",
        "def load_dataset(path, num_examples=None):\n",
        "    # creating cleaned input, output pairs\n",
        "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "    \n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n"
      ],
      "metadata": {
        "id": "Qc2sPnc1X7Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick 30k sample\n",
        "num_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n"
      ],
      "metadata": {
        "id": "eDiYXMxxYDth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maximum length of the input sentence is 16 tokens, output sentense is 11 tokens."
      ],
      "metadata": {
        "id": "VMSa_4uPYWdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor_train.shape, input_tensor_val.shape, target_tensor_train.shape, target_tensor_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW5brrgFYOSS",
        "outputId": "32287630-69fb-4ab1-bd1d-9d9232b15e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((24000, 16), (6000, 16), (24000, 11), (6000, 11))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each sentence transformed into a list of integers. Each element (int) denotes a token in the sentence."
      ],
      "metadata": {
        "id": "P2FUfL-kYxPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH3wzJcrYTgi",
        "outputId": "6ab26ee6-6ec9-47f2-b609-9648626d0e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   1,   83,   11, ...,    0,    0,    0],\n",
              "       [   1,    6, 1410, ...,    0,    0,    0],\n",
              "       [   1,   23,  670, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [   1,  542, 1934, ...,    0,    0,    0],\n",
              "       [   1,    6,  580, ...,    0,    0,    0],\n",
              "       [   1,    4,  261, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First sentence. <start> token encoded as 1, <end> token encoded as 2. Zeros are paddings at vacant token places."
      ],
      "metadata": {
        "id": "nEo-l3DNZZZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNrrsg1SY3ha",
        "outputId": "544849e2-3c6a-4d6f-9d48-5be8e04d0580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   1,   83,   11, 1041,    3,    2,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print(\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "      \n",
        "convert(inp_lang, input_tensor_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVp0j7NbZYQp",
        "outputId": "f59159b1-270c-4f8d-e439-6239fb0298ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 ----> <start>\n",
            "83 ----> tienes\n",
            "11 ----> que\n",
            "1041 ----> regresar\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First 10 tokens in the vocabulary."
      ],
      "metadata": {
        "id": "5nsSJaobaSID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(inp_lang.word_index.items())[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQV9wPqtZ2rR",
        "outputId": "523cb9b5-851f-4529-c5de-9ab291deb9ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<start>', 1),\n",
              " ('<end>', 2),\n",
              " ('.', 3),\n",
              " ('tom', 4),\n",
              " ('?', 5),\n",
              " ('¿', 6),\n",
              " ('es', 7),\n",
              " ('no', 8),\n",
              " ('el', 9),\n",
              " ('a', 10)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last 10 tokens in the vocabulary"
      ],
      "metadata": {
        "id": "0enJwz2GagWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(inp_lang.word_index.items())[-8:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPidviXmaYtf",
        "outputId": "fc48646e-1bbe-4126-e9d0-cc35115b49c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('caminamos', 9406),\n",
              " ('divertir', 9407),\n",
              " ('divertiremos', 9408),\n",
              " ('divertirnos', 9409),\n",
              " ('decepcionaremos', 9410),\n",
              " ('viviremos', 9411),\n",
              " ('reyes', 9412),\n",
              " ('perderemos', 9413)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be able to process sentences by neural network , need to encode sentences."
      ],
      "metadata": {
        "id": "RTsu1xnTauJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = preprocess_sentence('Todo sobre mi madre.')\n",
        "inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtEsdVFgajLC",
        "outputId": "820e6025-1749-4723-b35a-de34d05c0ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 74, 514, 19, 237, 3, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "\n",
        "''' Compress the 9414 dimensional input vectors into 256 dimensional vectors. '''\n",
        "embedding_dim = 256\n",
        "''' The dimension of the hidden state/vector. '''\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1 # 9414 \n",
        "vocab_tar_size = len(targ_lang.word_index)+1 # 4935 \n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "1hleOmaga1N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this implementation, during training the 'Encoder' class gets a (64, 16) tensor as an input, and it gives out a (64, 16, 1024) tensor as an output, regrdless of how many words the inputs have. That means the class gets the whole sentence as a sequence of integers, and gives out a 1024-dim vector every times step, I mean each token. "
      ],
      "metadata": {
        "id": "Fe-VjHDKbVNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz # 64 \n",
        "    self.enc_units = enc_units  # 24000 // 64 = 375 \n",
        "    \n",
        "    '''\n",
        "      As I explained in the last article, you propagate input 9414 dimensional vectors to 256 embedding vectors. \n",
        "    '''\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # (9414, 256)\n",
        "    \n",
        "    '''\n",
        "    We use a RNN model named GRU for this seq2seq translation model. \n",
        "    All you have to keep in mind is, in this implentation, at time step t, one GRU cell takes 'embedding_dim'(=256) \n",
        "    dimensional vector as an input, and gives out a 16 dimensional (the maximum size of input sentences) output vector \n",
        "    and succeeds a hidden state/vector to the next GRU cell.  \n",
        "    '''\n",
        "    \n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units, # 1024 *the dimension of the hidden vector/state. \n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    \n",
        "    '''\n",
        "     tf.keras.layers.GRU class gets [batch, timesteps, feature] sized tensors as inputs. \n",
        "     https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU\n",
        "    '''\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "\n",
        "'''\n",
        "  You construct an 'Encoder' calss as below. \n",
        "  One cell get a 9414 dimensional one-hot vector, and i\n",
        "'''\n",
        "encoder = Encoder(vocab_inp_size, # 9414 \n",
        "                  embedding_dim, # 256 \n",
        "                  units, # 1024 \n",
        "                  BATCH_SIZE # 24000 \n",
        "                 )\n"
      ],
      "metadata": {
        "id": "KbVQz6n9bIFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    '''\n",
        "      In the decoder part, you get an embedding vector for an input token every time step. \n",
        "      You have to calculate attentions using this class at EVERY TIME STEP. \n",
        "      'query' is the hidden state of an RNN cell at the time in the decoder part, whose size is (batch_size, 1, 1024).\n",
        "      'values' is the outputs of the encoder part, whose size is (batch_size, 16, 1024). \n",
        "      (*The length of the input is not necessarily 16.)\n",
        "      \n",
        "      Attention mehcanism calculates relevances of a query and values with a certain function. \n",
        "      There are several functions for calculating the relevances, and in this implementation \n",
        "      we use Bahdanau\"s additive style. \n",
        "    '''\n",
        "    \n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    \n",
        "    '''\n",
        "      In this implementation, you always need to consider time steps. \n",
        "    '''\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    '''\n",
        "      You get the attentions between the query and outputs of the encoder below.\n",
        "      In short, you compare the a word in the decoder with the input. \n",
        "      This is equivalent to finding the corresponding words in the original language, \n",
        "      when you are going to write a word in the target language. \n",
        "    '''\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(query_with_time_axis)))\n",
        "\n",
        "    '''\n",
        "      You normalize the score calculated above with a softmax function so that the usm of its values is 1. \n",
        "    '''\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    '''\n",
        "      You reweight the outputs of the encoder with attention scores.\n",
        "      The shape of the resulitng 'context_vector' is (64, 16, 1024)\n",
        "    '''\n",
        "    context_vector = attention_weights * values \n",
        "    '''\n",
        "      You calculate the weighted average of the reweighted vectors above. \n",
        "      Thus the size of the shape of the resulting 'context_vector' is (64, 1024). \n",
        "    '''\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1) # You take a weighted average of c\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "KOGxnHYzbkyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    '''\n",
        "     As well as 'Encoder' class, the shape of inputs of 'Decoder' is [batch, timesteps, feature]. \n",
        "     https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU\n",
        "     But you have to keep it in mind that you input a token every time step, the input is \n",
        "     (batch_size, 1, embedding_dim). \n",
        "    '''\n",
        "    \n",
        "    '''\n",
        "     You first calculate a 'context_vector' by comparing the hidden layer of the LAST cell, \n",
        "     with the outputs of the encoder because you use Bahdanau's additive style attention mechanism. \n",
        "     You usually use the hidden layer of the current cell.\n",
        "     \n",
        "    '''\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    '''\n",
        "     You combine the 'context_vector' with the embedding vector of the decoder input at this time step. \n",
        "     And the RNN cell at current time step gives out a predicted word, given the combined input. \n",
        "    '''\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    \n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "    '''\n",
        "      x: a vector whose dimension is the size of the output vocabulary size. The index of the maximum\n",
        "         element of this vector is the index of the predicted word at this time step. \n",
        "      state: the hidden state at this time step. This is the query of the next time step in Bahdanau's \n",
        "             additive style. \n",
        "    '''\n",
        "    return x, state, attention_weights\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "914QHz98buQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "CaL0CfrLb1v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "metadata": {
        "id": "_6jalPZsb5CT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  '''\n",
        "    You input a (batch size, max input length) (=(64, 16)) tensor as an input\n",
        "    and a (batch size, max output length) (=(64, 11)) as an output, and get a loss. \n",
        "  '''\n",
        "    \n",
        "  with tf.GradientTape() as tape:\n",
        "    '''    \n",
        "      You put a batch of input sentences as a (64, 16) tensor. \n",
        "    '''\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    '''\n",
        "      You pass the last hidden state/vector of the encoder to the decoder as its \n",
        "      inittial layer.\n",
        "    '''\n",
        "    \n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "    \n",
        "    '''\n",
        "    In the encoder part you pass the whole sentence as an input, \n",
        "    whereas in the decoder part, you pass a word every time step in the loop below.  \n",
        "    '''\n",
        "    \n",
        "    '''\n",
        "      The loop below shows that you \n",
        "    '''\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      \n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, \n",
        "                                           dec_hidden, \n",
        "                                           enc_output) # You need encoder outputs to calculate attentions. \n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  '''\n",
        "  Updating the weigths with the three lines below.   \n",
        "  '''\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "metadata": {
        "id": "6zNAr1k5b7k2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  '''\n",
        "   You initialize the 'unit' dimensional hidden layer (1024 dimensional) as a 'unit' dimensional zero vector. \n",
        "  '''\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    '''\n",
        "     You input a (batch size, max input length) (=(64, 16)) matrix as an input\n",
        "     and a (batch size, max output length) (=(64, 11)) as an output, and get a loss. \n",
        "     'enc_hidden' is the last 'units' dimensional hidden state vector (1024 dimensional) of the encoder. \n",
        "    '''\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKO6l7IpcGHL",
        "outputId": "63941621-3b02-4bc8-83f9-5d0812287522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 4.5063\n",
            "Epoch 1 Batch 100 Loss 2.1224\n",
            "Epoch 1 Batch 200 Loss 1.7876\n",
            "Epoch 1 Batch 300 Loss 1.6997\n",
            "Epoch 1 Loss 2.0316\n",
            "Time taken for 1 epoch 119.84375071525574 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.6021\n",
            "Epoch 2 Batch 100 Loss 1.3998\n",
            "Epoch 2 Batch 200 Loss 1.2826\n",
            "Epoch 2 Batch 300 Loss 1.2212\n",
            "Epoch 2 Loss 1.3813\n",
            "Time taken for 1 epoch 104.93744564056396 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.0259\n",
            "Epoch 3 Batch 100 Loss 1.0805\n",
            "Epoch 3 Batch 200 Loss 0.9038\n",
            "Epoch 3 Batch 300 Loss 0.8294\n",
            "Epoch 3 Loss 0.9576\n",
            "Time taken for 1 epoch 103.60822772979736 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.5659\n",
            "Epoch 4 Batch 100 Loss 0.6945\n",
            "Epoch 4 Batch 200 Loss 0.7058\n",
            "Epoch 4 Batch 300 Loss 0.5952\n",
            "Epoch 4 Loss 0.6519\n",
            "Time taken for 1 epoch 103.36160707473755 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.4489\n",
            "Epoch 5 Batch 100 Loss 0.3336\n",
            "Epoch 5 Batch 200 Loss 0.4785\n",
            "Epoch 5 Batch 300 Loss 0.4565\n",
            "Epoch 5 Loss 0.4431\n",
            "Time taken for 1 epoch 101.60948491096497 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.2772\n",
            "Epoch 6 Batch 100 Loss 0.2831\n",
            "Epoch 6 Batch 200 Loss 0.3567\n",
            "Epoch 6 Batch 300 Loss 0.2887\n",
            "Epoch 6 Loss 0.3071\n",
            "Time taken for 1 epoch 102.70002770423889 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.1485\n",
            "Epoch 7 Batch 100 Loss 0.1977\n",
            "Epoch 7 Batch 200 Loss 0.2689\n",
            "Epoch 7 Batch 300 Loss 0.1972\n",
            "Epoch 7 Loss 0.2162\n",
            "Time taken for 1 epoch 101.64530849456787 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.1826\n",
            "Epoch 8 Batch 100 Loss 0.1309\n",
            "Epoch 8 Batch 200 Loss 0.1740\n",
            "Epoch 8 Batch 300 Loss 0.1519\n",
            "Epoch 8 Loss 0.1603\n",
            "Time taken for 1 epoch 101.96675729751587 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.1279\n",
            "Epoch 9 Batch 100 Loss 0.1074\n",
            "Epoch 9 Batch 200 Loss 0.1224\n",
            "Epoch 9 Batch 300 Loss 0.1450\n",
            "Epoch 9 Loss 0.1246\n",
            "Time taken for 1 epoch 101.46724009513855 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0663\n",
            "Epoch 10 Batch 100 Loss 0.0948\n",
            "Epoch 10 Batch 200 Loss 0.0844\n",
            "Epoch 10 Batch 300 Loss 0.1317\n",
            "Epoch 10 Loss 0.1029\n",
            "Time taken for 1 epoch 102.23549151420593 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(sentence):\n",
        "  '''\n",
        "    Preparing an array to display attention scores. \n",
        "  '''\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  '''\n",
        "    Preparing inputs and outputs. \n",
        "  '''\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  '''\n",
        "    Initializing the hidden state of the encoder and getting outputs \n",
        "    the encoder.\n",
        "  '''\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "    \n",
        "    \n",
        "    '''\n",
        "      You get a 'prediction' at every time step in the decoder part. \n",
        "      The index of the maximum element is the index of the predicted word. \n",
        "    '''\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "    \n",
        "    '''\n",
        "      When the decoder generate the token '<end>', RNNs stop decoding. \n",
        "    '''\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot\n"
      ],
      "metadata": {
        "id": "HLriQ3xtcI8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "FYj9LvKNcVhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  '''\n",
        "    You delete unnecessary attention scores below, and display the reduced attention scores. \n",
        "  '''\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "metadata": {
        "id": "G16b7PEacWQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# restoring checkpoint_dir \n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qYyCbi8ccL2",
        "outputId": "cc3b2305-33fc-44bf-b2fb-c79c9b561527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f7580acb9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(u'hace mucho frio aqui.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "UbeGO2VNccuA",
        "outputId": "5235dc8d-d328-40f7-c046-7b29a19bc387"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> hace mucho frio aqui . <end>\n",
            "Predicted translation: it s very cold here . <end> \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hM2iIxURhn0p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}